# ============================================================
# FIREBASE CONFIGURATION (REQUIRED)
# ============================================================
# Get these from Firebase Console (https://console.firebase.google.com)
# 
# FIREBASE_PROJECT_ID:
#   - Go to Firebase Console
#   - Select your project
#   - Go to Settings (gear icon) → Project settings
#   - Copy "Project ID" field
#
# GOOGLE_APPLICATION_CREDENTIALS:
#   - Go to Settings → Service accounts
#   - Click "Generate new private key"
#   - Save the JSON file as: config/serviceAccountKey.json (in this repo)
#   - Local dev: use path below (relative to project root when you run the server)
#   - Docker: mount the file and set GOOGLE_APPLICATION_CREDENTIALS=/app/config/serviceAccountKey.json

FIREBASE_PROJECT_ID=your-firebase-project-id
GOOGLE_APPLICATION_CREDENTIALS=./config/serviceAccountKey.json

# ============================================================
# LLM CONFIGURATION - PERPLEXITY (PRIMARY)
# ============================================================
# Perplexity is the primary LLM provider (cheaper, built-in web search)
# 
# Get API key:
#   1. Go to https://www.perplexity.ai/settings/api
#   2. Click "Create API Key"
#   3. Copy the key (starts with "pplx-")
#
# Model options (see https://docs.perplexity.ai/docs/getting-started/models):
#   - sonar (recommended - fast + cheap)
#   - sonar-pro (slower, better quality)

PERPLEXITY_API_KEY=pplx-your-api-key-here
PERPLEXITY_MODEL=sonar

# ============================================================
# LLM CONFIGURATION - OPENAI (FALLBACK ONLY)
# ============================================================
# Only set these if Perplexity is not available.
# Leave empty if using Perplexity.
#
# Get API key:
#   1. Go to https://platform.openai.com/api-keys
#   2. Click "Create new secret key"
#   3. Copy the key (starts with "sk-proj-")
#
# Model options:
#   - gpt-3.5-turbo (fastest, cheapest)
#   - gpt-4 (smartest, most expensive)

OPENAI_API_KEY=
OPENAI_MODEL=gpt-3.5-turbo

# ============================================================
# LANGSMITH CONFIGURATION (OPTIONAL - FOR DEBUGGING)
# ============================================================
# LangSmith helps debug LLM chains (shows latency, tokens, costs)
# Free tier: 5k traces/month
# 
# Get API key:
#   1. Go to https://smith.langchain.com/
#   2. Sign up (free)
#   3. Go to Settings → API keys
#   4. Click "Create API key"
#   5. Copy the key

LANGSMITH_API_KEY=
LANGSMITH_ENABLED=false

# ============================================================
# GRAPH CONFIGURATION
# ============================================================
# These control how graphs behave

# Maximum seconds a graph can run before timing out
# Keep reasonable (not too short, not too long)
GRAPH_TIMEOUT=30

# Maximum candidates to fetch from Firestore per query
# Higher = more results but slower (and more expensive LLM calls)
# Lower = faster but might miss good matches
MAX_CANDIDATES=100

# ============================================================
# SECURITY CONFIGURATION
# ============================================================
# AI_SERVICE_TOKEN (optional but recommended in production)
# Shared secret that your JS backend sends as:
#   Authorization: Bearer <token>
# If empty, the API will accept unauthenticated requests.
AI_SERVICE_TOKEN=
# ALLOWED_ORIGINS (optional if you want dynamic CORS)
# Comma-separated list of allowed origins, e.g.:
#   ALLOWED_ORIGINS=http://localhost:5173,https://example.com
# Current code uses a static list in src/server.py; use this when you
# decide to load origins from environment instead.
ALLOWED_ORIGINS=

# ============================================================
# SERVER CONFIGURATION
# ============================================================
# Port to run the FastAPI server on
# Default 8000 is fine for most cases
PORT=8000

# Host to bind to
# 0.0.0.0 = accept connections from anywhere (needed for Docker)
# 127.0.0.1 = only local connections
HOST=0.0.0.0

# Enable debug logging
# Set to true for development (verbose logging)
# Set to false for production (less noise)
DEBUG=false
